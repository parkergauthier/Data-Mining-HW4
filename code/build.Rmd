---
title: "Data-Mining-HW4"
author: "Parker Gauthier"
date: "4/24/2022"
output: md_document
---

```{r include = FALSE}
#Appropriate packages
if (!("librarian" %in% rownames(utils::installed.packages()))) {
  utils::install.packages("librarian")
}

librarian::shelf(
    cran_repo = "https://cran.microsoft.com/",
    ask = FALSE,
    here,
    tidyverse,
    stargazer,
    mosaic,
    dplyr,
    ggcorrplot,
    kableExtra,
    lmtest,
    Metrics,
    ggthemes,
    arules,
    arulesViz,
    igraph,
    LICORS,
    foreach,
    plotly,
    factoextra
  )

here::i_am("code/build.Rmd")


```

# 1.) Clustering and PCA

```{r include=FALSE}
#Reading in data
wine = read.csv(here("data/wine.csv"))

#Turning red vs white into a dummy variable, 1 for red, 0 for white
wine$color = ifelse(wine$color == "red", 1, 0)
```

## PCA

To begin this problem, lets see what we can find from looking merely at the correlations between the traits of wines using a heatmap:
```
```
```{r message=FALSE, warning=FALSE, echo=FALSE}

ggcorrplot::ggcorrplot(cor(wine), hc.order=TRUE)
```
```
```
There are some interesting characteristics revealed by the plot above.  We can see that the measurement for residual sugar is highly correlated with the amount of sulfur dioxide in a wine. Color on the other hand (coded 1 for reds and 0 for whites), seems to be negatively correlated with these.  This seems to suggest that white wines are sweeter and contains less sulfur dioxide.  Additionally, the quality of a wine seems to be associated with higher alcohol content and lower values of volatile acidity. 

```
```

Now, lets move onto our Principle Components Analysis in order to predict the quality and color of wines based on their chemical makeup. We will breakdown our wines into four principle components.  Below shows that these components explain roughly 73% of the proportional variance in our data:

```{r message=FALSE, warning=FALSE, echo=FALSE}
# Removing the categories we want to predict
chems = wine %>% 
  select(-quality, -color)

# Building components
PCAwine = prcomp(chems, scale = TRUE, rank = 4)

summary(PCAwine)
```
```
```
For further investigation, lets look at how each trait factors into each of our components:

```{r message=FALSE, warning=FALSE, echo=FALSE}
# Displaying traits in each component
pca_summary = PCAwine$rotation %>% 
  as.data.frame() %>% 
  round(2) %>% 
  rownames_to_column("Trait") %>% 
  arrange(desc(PC1)) 

kable(pca_summary)
```
```
```
Above, we can see that PC1 places high positive values on amounts of sulfur dioxide and high negative values volatile acidity.  It is hard to say explicitly what PC1 is contrasting, but it could be what gives a wine its appearance. PC2, on the other hand, gives high positive values to density, residual sugar, and fixed acidity.  These seem to contrast a wine's flavor characteristics.

```{r message=FALSE, warning=FALSE, echo=FALSE}
# Merging PCAs with wine data set
winePCA = merge(wine, PCAwine$x[,1:4], by = "row.names")
```

```
```
Next, by plotting PC1 vs PC2 and shading by color, we can see that PCA enables us to distinguish the colors of wine based off the features in our data set:
```
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
 ggplot(data = winePCA, aes(x = PC1, y = PC2, color = factor(color))) +
   geom_point() +
  scale_color_hue(labels = c("White","Red"))+
  labs(color= "Color")
```
```
```
Finally, we will see that the grouping of qualities is nearly non-existent by plotting PC1 vs PC2 and shading by quality:
```{r message=FALSE, warning=FALSE, echo=FALSE}
 ggplot(data = winePCA, aes(x = PC1, y = PC2, color = factor(quality))) +
   geom_point() +
  labs(color = "Quality")
```

## K-Means Clustering
For our next analysis, we will use K-means to see if we can identify clusters based off quality and color.  To start, we will remove color and quality from our data set as these are the clusters we are trying to identify.  We will then scale the data to feed in the appropriate z-scores into our clustering algorithm. To see if we can identify clusters by color we will set the number of clusters to 2. For quality the number of clusters will be 7 (even though the ratings are from 1-10, there are only 7 different rating in our data set). Finally, we will plot the clusters using various features from our data set and compare them to plots of the actual quality and color.  These plots are displayed below:

### Color
```{r message=FALSE, warning=FALSE, echo=FALSE}
#Centering and scaling
chem = wine %>% 
  select(-quality, -color)

chem = scale(chem, center= TRUE, scale= TRUE)

#Extracting centers and scales
mu = attr(chem, "scaled:center")
sigma = attr(chem, "scaled:center")


#K-means with two clusters and 30 starts
clust1 = kmeanspp(chem, 2, nstart = 50)

clusters = as.data.frame(clust1$cluster)

wine_pred = merge(wine, clusters, by = "row.names")


```
```
```

#### Residual sugar by alcohol content colored by cluster in the first and actual color in the second:
```
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
wine_pred %>% 
  ggplot(aes(x = alcohol, y= residual.sugar, color = factor(`clust1$cluster`))) +
  geom_point() +
  labs(x = "Alcohol Content", y = "Residual Sugar", color = "Cluster", title = "Residual Sugar vs Alcohol Content", subtitle = "Colored by Cluster")
```
```
```
```{r message=FALSE, warning=FALSE, echo=FALSE}

wine_pred %>% 
  ggplot(aes(x = alcohol, y= residual.sugar, color = factor(color))) +
  geom_point()+
  labs(x = "Alcohol Content", y = "Residual Sugar", color = "Cluster", title = "Residual Sugar vs Alcohol Content", subtitle = "Colored by Color of Wine")


```
```
```
#### Total sulfur dioxide by volatile acidity:
```
```
```{r message=FALSE, warning=FALSE, echo=FALSE}

wine_pred %>% 
  ggplot(aes(x = volatile.acidity, y= total.sulfur.dioxide, color = factor(`clust1$cluster`))) +
  geom_point() +
  labs(x = "Volatile Acidity", y = "Total Sulfer Dioxide", color = "Color", title = "Total Sulfer Dioxide by Volatile Acidity", subtitle = "Colored by Cluster")
```
```
```
```{r message=FALSE, warning=FALSE, echo=FALSE}

wine_pred %>% 
  ggplot(aes(x = volatile.acidity, y= total.sulfur.dioxide, color = factor(color))) +
  geom_point()+
  labs(x = "Volatile Acidity", y = "Total Sulfer Dioxide", color = "Color", title = "Total Sulfer Dioxide by Volatile Acidity", subtitle = "Colored by Color of Wine")
```
```
```
#### Density by citric acid:
```
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
wine_pred %>% 
  ggplot(aes(x = density, y= citric.acid, color = factor(`clust1$cluster`))) +
  geom_point() +
  labs(x = "Citric Acid", y = "Density", color = "Cluster", title = "Density vs Citric Acid", subtitle = "Colored by Cluster")
```
```
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
wine_pred %>% 
  ggplot(aes(x = density, y= citric.acid, color = factor(color))) +
  geom_point() +
  labs(x = "Citric Acid", y = "Density", color = "Color", title = "Density vs Citric Acid", subtitle = "Colored by Color of Wine")
```
```
```

### Quality
```{r include = FALSE}
clust1 = kmeanspp(chem, 7, nstart = 50)

clusters = as.data.frame(clust1$cluster)

wine_pred = merge(wine, clusters, by = "row.names")
```

#### Residual sugar by alcohol content colored by cluster in the first and actual rating in the second:
```{r message=FALSE, warning=FALSE, echo=FALSE}
wine_pred %>% 
  ggplot(aes(x = alcohol, y= residual.sugar, color = factor(`clust1$cluster`))) +
  geom_point() +
  labs(x = "Alcohol Content", y= "Residual Sugar", color = "Cluster", title = "Residual Sugar vs Alcohol Content", subtitle = "Colored by Cluster")
```
```
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
wine_pred %>% 
  ggplot(aes(x = alcohol, y= residual.sugar, color = factor(quality))) +
  geom_point() +
  labs(x = "Alcohol Content", y= "Residual Sugar", color = "Rating", title = "Residual Sugar vs Alcohol Content", subtitle = "Colored by Rating")
```

#### Total sulfur dioxide by volatile acidity:
```{r message=FALSE, warning=FALSE, echo=FALSE}

wine_pred %>% 
  ggplot(aes(x = volatile.acidity, y= total.sulfur.dioxide, color = factor(`clust1$cluster`))) +
  geom_point() +
  labs(x = "Volatile Acidity", y = "Total Sulfur Dioxide", color = "Cluster", title = "Total Sulfur Dioxide by Volatile Acidity", subtitle = "Colored by Cluster")
```
```
```
```{r message=FALSE, warning=FALSE, echo=FALSE}

wine_pred %>% 
  ggplot(aes(x = volatile.acidity, y= total.sulfur.dioxide, color = factor(quality))) +
  geom_point() +
  labs(x = "Volatile Acidity", y = "Total Sulfur Dioxide", color = "Rating", title = "Total Sulfur Dioxide by Volatile Acidity", subtitle = "Colored by Rating")
```
```
```
#### Density by citric acid:

```{r message=FALSE, warning=FALSE, echo=FALSE}
wine_pred %>% 
  ggplot(aes(x = density, y= citric.acid, color = factor(`clust1$cluster`))) +
  geom_point() +
  labs(x = "Density", y = "Citric Acid", color= "Cluster", title = "Density by Citric Acid", subtitle = "Colored by Cluster")
```
```
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
wine_pred %>% 
  ggplot(aes(x = density, y= citric.acid, color = factor(quality))) +
  geom_point() + 
  labs(x = "Density", y = "Citric Acid", color= "Rating", title = "Density by Citric Acid", subtitle = "Colored by Rating")
```
```
```

Our plots above suggest that the k-means algorithm does well in separating clusters by color but does not do well with respect to quality.  This seems to highlight the subjectivity of the quality scores.  Quality scores seem to have a very arbitrary distribution and there is no clear relationship with the other features in the data set.


## Conclusion
We have seen that both PCA and K-means clustering do well in segmenting our data by color but not by quality.  With this in mind, it makes more sense to use PCA for our data set when it comes to dimensionality reduction. By consolidating our covariates into a Euclidian space, PCA does very well in showing contrasts and the color variable in our data has two distinct factors we can emphasize.   While K-means clustering can help us see contrasts in data as well, it does so in a classification space which can sway the definitions of our clusters away from the actual colors since it is sensitive to sparsity/noise in our data.


# 2.) Market Segmentation
```{r include = FALSE}
market = read.csv(here("data/social_marketing.csv"))

```
For this analysis, we will try to determine different market segments utilizing user posts categorized by topic (observations with values for adult and spam are removed from this data set, along with the columns for chatter and photo sharing in order to help better identify specific segments).  To begin, let us visualize the correlations in our data set to help build our intuition as to  where clusters may form:
```
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
corr_market = market %>% filter(spam == 0 & adult ==0) %>% select(-X, -chatter, -photo_sharing, -adult, -spam)

ggcorrplot::ggcorrplot(cor(corr_market), hc.order=TRUE)
```
```
```
From the plot above we can see that there are some strong associations based off the sentiment of the post. We can almost identify our clusters just from this graph. For example, outdoors, personal fitness, and nutrition all seem to be highly correlated.  We can expect certain clusters will be reflective of correlations such as these.  To help us find the optimal amount of clusters we will refer to the elbow plot shown below:
```
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
market_prep = market %>% filter(spam == 0 & adult ==0) %>% select(-X, -chatter, -photo_sharing, -adult, -spam)

fviz_nbclust(market_prep, kmeans, method = "wss") +
  labs(subtitle = "Elbow Method")
```
```
```
From what we see from this plot, we appear to get the greatest diminishing returns around seven clusters.  This is the amount we will use going forward.
```{r message=FALSE, warning=FALSE, echo=FALSE}
#K-means with two clusters and 25 starts
clust = kmeanspp(market_prep, 7, nstart = 25)

```
```
```
Now, to help give us some intuition on how the clusters are distinguished, lets look the clusters' centers and the five types of tweets they send the most:
```
```
#### Cluster Center 1
```{r message=FALSE, warning=FALSE, echo=FALSE}
clust1 = as.data.frame(clust$centers[1,])

clust1 %>% rename("Center" = `clust$centers[1, ]`) %>% arrange(desc(Center)) %>% head(5) 
```
```
```
#### Cluster Center 2
```{r message=FALSE, warning=FALSE, echo=FALSE}
clust2 = as.data.frame(clust$centers[2,])

clust2 %>% rename("Center" = `clust$centers[2, ]`) %>% arrange(desc(Center)) %>% head(5) 
```
```
```
####  Cluster Center 3
```{r message=FALSE, warning=FALSE, echo=FALSE}
clust3 = as.data.frame(clust$centers[3,])

clust3 %>% rename("Center" = `clust$centers[3, ]`) %>% arrange(desc(Center)) %>% head(5) 
```
```
```
####  Cluster Center 4
```{r message=FALSE, warning=FALSE, echo=FALSE}
clust4 = as.data.frame(clust$centers[4,])

clust4 %>% rename("Center" = `clust$centers[4, ]`) %>% arrange(desc(Center)) %>% head(5) 
```

```
```
####  Cluster Center 5
```{r message=FALSE, warning=FALSE, echo=FALSE}
clust5 = as.data.frame(clust$centers[5,])

clust5 %>% rename("Center" = `clust$centers[5, ]`) %>% arrange(desc(Center)) %>% head(5) 
```

```
```
####  Cluster Center 6
```{r message=FALSE, warning=FALSE, echo=FALSE}
clust6 = as.data.frame(clust$centers[6,])

clust6 %>% rename("Center" = `clust$centers[6, ]`) %>% arrange(desc(Center)) %>% head(5) 
```

```
```
#### Cluster Center 7
```{r message=FALSE, warning=FALSE, echo=FALSE}
clust7 = as.data.frame(clust$centers[7,])

clust7 %>% rename("Center" = `clust$centers[7, ]`) %>% arrange(desc(Center)) %>% head(5) 
```
```
```
Our clusters have given us some fairly intuitive results.  We could infer a student classification for the cluster centering with high amounts of university and online gaming tweets. Furthermore, fitness-buffs are likely to fall in the cluster associating personal fitness and nutrition. Going forward, we can assign the users their respective clusters and market to them accordingly.



# 3.) Association Rules for Grocery Purchases

In this problem, we will explore and analyze various association rules for grocery purchases. To construct these rules, thresholds will be set accordingly: support at 0.01 in order to keep items that may not be purchased much on their own but may be purchased with other goods. Confidence at 0.2 to help filter out weaker associations. And finally, the maximum length will be set to 3 to ensure we are accounting for the bundling of purchases.  
```
```
Below displays a couple tables of interest:
```{r include = FALSE}
groceries = scan(here("data/groceries.txt"), what = "", sep = "\n")

groceries = strsplit(groceries, ",")

grocery_trans = as(groceries, "transactions")

groc_rules = apriori(grocery_trans, parameter = list(support=.01, confidence = .2, maxlen=3))

sub1 = subset(groc_rules, subset = confidence > .2 & support > .01)

rules_frame= data.frame(inspect(sub1))
```

```{r  include = FALSE}
rules_frame %>% arrange(desc(lift)) %>%  head(10) %>% kable(caption = "Rules Arranged in Descending Order by Lift")
```


```
```
When looking at the above table ordered by lift, we can see that certain bundles of goods can greatly increase the chances of buying other particular goods.  As an example, when purchasing citrus fruit and other vegetables, a consumer is much more likely to purchase root vegetables.  This could be the result of the items being located in the same area of the store, or that they all go into a particular recipe.  Another interesting takeaway from this table is that root vegetables seem to generally be associated with high amounts of lift.  Root vegetables seem to be purchased with other goods in far greater amounts than on their own, suggesting they are preferred as recipe components.

```
```
Our next table, ordered by confidence, shows us some other interesting features of grocery purchases.  The most notable, is that purchases of other goods greatly implies purchases of whole milk.  

```{r include = FALSE}

rules_frame %>% arrange(desc(confidence)) %>%  head(10) %>% kable(caption = "Rules Arranged in Descending Order by Confidence")

```
```
```
Finally, to visualize these connections, we will construct a network linking the purchases in our data.  Each node will represent an item purchased and they will be connected by the other items purchased with them.  The size of the nodes will correspond with its degree (in other words, the number of other goods purchased with it) and the color of the nodes will be by the modularity class showing the associations.  We can see that whole milk and other vegetables have the greatest number of connections.  Furthermore, there are some distinct groups of similar items.

```{r include=FALSE}

saveAsGraph(sub1, file = "groceries.graphml")

```
