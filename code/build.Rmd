---
title: "Data-Mining-HW4"
author: "Parker Gauthier"
date: "4/24/2022"
output: md_document
---

```{r include = FALSE}
#Appropriate packages
if (!("librarian" %in% rownames(utils::installed.packages()))) {
  utils::install.packages("librarian")
}

librarian::shelf(
    cran_repo = "https://cran.microsoft.com/",
    ask = FALSE,
    here,
    tidyverse,
    stargazer,
    mosaic,
    dplyr,
    ggcorrplot,
    kableExtra,
    lmtest,
    Metrics,
    ggthemes,
    arules,
    arulesViz,
    igraph,
    LICORS,
    foreach,
    plotly
  )

here::i_am("code/build.Rmd")


```

# 1.) Clustering and PCA

```{r include=FALSE}
#Reading in data
wine = read.csv(here("data/wine.csv"))

#Turning red vs white into a dummy variable, 1 for red, 0 for white
wine$color = ifelse(wine$color == "red", 1, 0)
```

## PCA

To begin this problem, lets see what we can find from looking merely at the correlations between the traits of wines using a heatmap:
```{r message=FALSE, warning=FALSE, echo=FALSE}

ggcorrplot::ggcorrplot(cor(wine), hc.order=TRUE)
```
```
```
There are some interesting characteristics revealed by the plot above.  We can see that the measurement for residual sugar is highly correlated with the amount of sulfur dioxide in a wine. Color on the other hand (coded 1 for reds and 0 for whites), seems to be negatively correlated with these.  This seems to suggest that white wines are sweeter and contains less sulfur dioxide.  Additionally, the quality of a wine seems to be associated with higher alcohol content and lower values of volatile acidity. 

```
```

Now, lets move onto our Principle Components Analysis in order to predict the quality and color of wines based on their chemical makeup. We will breakdown our wines into four principle components.  Below shows that these components explain roughly 73% of the proportional variance in our data:

```{r message=FALSE, warning=FALSE, echo=FALSE}
# Removing the categories we want to predict
chems = wine %>% 
  select(-quality, -color)

# Building components
PCAwine = prcomp(chems, scale = TRUE, rank = 4)

summary(PCAwine)
```
```
```
For further investigation, lets look at how each trait factors into each of our components:

```{r message=FALSE, warning=FALSE, echo=FALSE}
# Displaying traits in each component
pca_summary = PCAwine$rotation %>% 
  as.data.frame() %>% 
  round(2) %>% 
  rownames_to_column("Trait") %>% 
  arrange(desc(PC1)) 

kable(pca_summary)
```
```
```
Above, we can see that PC1 places high positive values on amounts of sulfur dioxide and high negative values volatile acidity.  It is hard to say explicitly what PC1 is contrasting, but it could be what gives a wine its appearance. PC2, on the other hand, gives high positive values to density, residual sugar, and fixed acidity.  These seem to contrast a wine's flavor characteristics.

```{r message=FALSE, warning=FALSE, echo=FALSE}
# Merging PCAs with wine data set
winePCA = merge(wine, PCAwine$x[,1:4], by = "row.names")
```

```
```
Next, by plotting PC1 vs PC2 and shading by color, we can see that PCA enables us to distinguish the colors of wine based off the features in our data set:
```{r message=FALSE, warning=FALSE, echo=FALSE}
 ggplot(data = winePCA, aes(x = PC1, y = PC2, color = factor(color))) +
   geom_point() +
  scale_color_hue(labels = c("White","Red"))+
  labs(color= "Color")
```
```
```
Finally, we will see that the grouping of qualities is nearly non-existent by plotting PC1 vs PC2 and shading by quality:
```{r message=FALSE, warning=FALSE, echo=FALSE}
 ggplot(data = winePCA, aes(x = PC1, y = PC2, color = factor(quality))) +
   geom_point() +
  labs(color = "Quality")
```

## K-Means Clustering
For our next analysis, we will use K-means to see if we can identify clusters based off quality and color.  To start, we will remove color and quality from our data set as these are the clusters we are trying to identify.  We will then scale the data to feed in the appropriate z-scores into our clustering algorithm. To see if we can identify clusters by color we will set the number of clusters to 2. For quality the number of clusters will be 7 (even though the ratings are from 1-10, there are only 7 different rating in our data set). Finally, we will plot the clusters using various features from our data set and compare them to plots of the actual quality and color.  These plots are displayed below:

### Color Plots
```{r message=FALSE, warning=FALSE, echo=FALSE}
#Centering and scaling
chem = wine %>% 
  select(-quality, -color)

chem = scale(chem, center= TRUE, scale= TRUE)

#Extracting centers and scales
mu = attr(chem, "scaled:center")
sigma = attr(chem, "scaled:center")


#K-means with two clusters and 30 starts
clust1 = kmeanspp(chem, 2, nstart = 50)

clusters = as.data.frame(clust1$cluster)

wine_pred = merge(wine, clusters, by = "row.names")



```
```
```

Residual sugar by alcohol content colored by cluster in the first and actual color in the second:
```{r message=FALSE, warning=FALSE, echo=FALSE}
wine_pred %>% 
  ggplot(aes(x = alcohol, y= residual.sugar, color = factor(`clust1$cluster`))) +
  geom_point()
```

```{r message=FALSE, warning=FALSE, echo=FALSE}

wine_pred %>% 
  ggplot(aes(x = alcohol, y= residual.sugar, color = factor(color))) +
  geom_point()


```
```
```
Total sulfur dioxide by volatile acidity:
```{r message=FALSE, warning=FALSE, echo=FALSE}

wine_pred %>% 
  ggplot(aes(x = volatile.acidity, y= total.sulfur.dioxide, color = factor(`clust1$cluster`))) +
  geom_point()
```

```{r message=FALSE, warning=FALSE, echo=FALSE}

wine_pred %>% 
  ggplot(aes(x = volatile.acidity, y= total.sulfur.dioxide, color = factor(color))) +
  geom_point()
```
```
```
Density by citric acid:

```{r message=FALSE, warning=FALSE, echo=FALSE}
wine_pred %>% 
  ggplot(aes(x = density, y= citric.acid, color = factor(`clust1$cluster`))) +
  geom_point()
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
wine_pred %>% 
  ggplot(aes(x = density, y= citric.acid, color = factor(color))) +
  geom_point()
```
```
```

### Quality
```{r include = FALSE}
clust1 = kmeanspp(chem, 7, nstart = 50)

clusters = as.data.frame(clust1$cluster)

wine_pred = merge(wine, clusters, by = "row.names")
```

Residual sugar by alcohol content colored by cluster in the first and actual color in the second:
```{r message=FALSE, warning=FALSE, echo=FALSE}
wine_pred %>% 
  ggplot(aes(x = alcohol, y= residual.sugar, color = factor(`clust1$cluster`))) +
  geom_point()
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
wine_pred %>% 
  ggplot(aes(x = alcohol, y= residual.sugar, color = factor(quality))) +
  geom_point()
```

Total sulfur dioxide by volatile acidity:
```{r message=FALSE, warning=FALSE, echo=FALSE}

wine_pred %>% 
  ggplot(aes(x = volatile.acidity, y= total.sulfur.dioxide, color = factor(`clust1$cluster`))) +
  geom_point()
```

```{r message=FALSE, warning=FALSE, echo=FALSE}

wine_pred %>% 
  ggplot(aes(x = volatile.acidity, y= total.sulfur.dioxide, color = factor(quality))) +
  geom_point()
```
```
```
Density by citric acid:

```{r message=FALSE, warning=FALSE, echo=FALSE}
wine_pred %>% 
  ggplot(aes(x = density, y= citric.acid, color = factor(`clust1$cluster`))) +
  geom_point()
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
wine_pred %>% 
  ggplot(aes(x = density, y= citric.acid, color = factor(quality))) +
  geom_point()
```
```
```

Our plots above suggest that the k-means algorithm does well in separating clusters by color but does not do well with respect to quality.  This seems to highlight the subjectivity of the quality scores.  Quality scores seem to have a very arbitrary distribution and there is no clear relationship with the other features in the data set.


## Conclusion
We have seen that both PCA and K-means clustering does well in segmenting our data by color but not by quality.  With this in mind, it makes more sense to use PCA for our data set when it comes to dimensionality reduction. By consolidating our covariates into a Euclidian space, PCA does very well in showing contrasts and the color variable in our data has two distinct factors we can emphasize.   While K-means clustering can help us see contrasts in data as well, it does so in a classification space which can be sway the definitions of our clusters away from the actual colors.

```{r message=FALSE, warning=FALSE, echo=FALSE}
# Running logit regressing on PCAs
glm_color = glm(color ~ PC1 + PC2 + PC3 + PC4, data = winePCA, family = "binomial")

#making predictions
pred_color = predict(glm_color, winePCA, type = "response")

yhat_color = ifelse(pred_color >= 0.5, 1, 0)

# Making confusion matrix to display true positive rate
confusion_color = table(Actual = winePCA$color, Predicted = yhat_color)

round(sum(diag(confusion_color))/sum(confusion_color) * 100, 2)

```


```{r message=FALSE, warning=FALSE, echo=FALSE}
#Running lm on quality
lm_qual = lm(quality ~ PC1 + PC2 + PC3 + PC4, data = winePCA)

winePCA = winePCA %>% 
  mutate(pred_qual = predict(lm_qual, winePCA, type = "response"))

winePCA %>% 
  ggplot(aes(x = factor(quality), y = pred_qual)) + 
  geom_boxplot(notch = TRUE, outlier.color = "blue") +
  theme_economist() +
  labs(x = "Actual", y = "Predicted", title= "Predicted vs Actual Wine Qualities")
```



# 2.) Market Segmentation
```{r include = FALSE}
market = read.csv(here("data/social_marketing.csv"))

```




# 3.) Association Rules for Grocery Purchases

In this problem, we will explore and analyze various association rules for grocery purchases. To construct these rules, thresholds will be set accordingly: support at 0.01 in order to keep items that may not be purchased much on their own but may be purchased with other goods. Confidence at 0.2 to help filter out weaker associations. And finally, the maximum length will be set to 3 to ensure we are accounting for the bundling of purchases.  
```
```
Below displays a couple tables of interest:
```{r message=FALSE, warning=FALSE, echo=FALSE}
groceries = scan(here("data/groceries.txt"), what = "", sep = "\n")

groceries = strsplit(groceries, ",")

grocery_trans = as(groceries, "transactions")

groc_rules = apriori(grocery_trans, parameter = list(support=.01, confidence = .2, maxlen=3))

sub1 = subset(groc_rules, subset = confidence > .2 & support > .01)

rules_frame= data.frame(inspect(sub1))

rules_frame
```


```
```
When looking at the above table ordered by lift, we can see that certain bundles of goods can greatly increase the chances of buying other particular goods.  As an example, when purchasing citrus fruit and other vegetables, a consumer is much more likely to purchase root vegetables.  This could be the result of the items being located in the same area of the store, or that they all go into a particular recipe.  Another interesting takeaway from this table is that root vegetables seem to generally be associated with high amounts of lift.  Root vegetables seem to be purchased with other goods in far greater amounts than on their own, suggesting they are preferred as recipe components.

```
```
Our next table, ordered by confidence, shows us some other interesting features of grocery purchases.  The most notable, is that purchases of other goods greatly implies purchases of whole milk.  

```{r message=FALSE, warning=FALSE, echo=FALSE}



```
```
```
Finally, to visualize these connections, we will construct a network linking the purchases in our data.  Each node will represent an item purchased and they will be connected by the other items purchased with them.  The size of the nodes will correspond with its degree (in other words, the number of other goods purchased with it) and the color of the nodes will be by the modularity class showing the associations.  We can see that whole milk and other vegetables have the greatest number of connections.  Furthermore, there are some distinct groups of similar items.

```{r include=FALSE}

saveAsGraph(sub1, file = "groceries.graphml")
```
